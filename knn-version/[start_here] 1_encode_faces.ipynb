{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Imports & Arguments set!\n",
      "[INFO] processing person [1/30] Roh_Moo-hyun\n",
      "[INFO] training using  Roh_Moo-hyun_0004.jpg\n",
      "Roh_Moo-hyun/Roh_Moo-hyun_0004.jpg was skipped and can't be used for training because detected 2 faces. \n",
      "[INFO] training using  Roh_Moo-hyun_0008.jpg\n",
      "[INFO] training using  Roh_Moo-hyun_0010.jpg\n",
      "[INFO] training using  Roh_Moo-hyun_0009.jpg\n",
      "[INFO] training using  Roh_Moo-hyun_0003.jpg\n",
      "[INFO] training using  Roh_Moo-hyun_0005.jpg\n",
      "[INFO] training using  Roh_Moo-hyun_0007.jpg\n",
      "[INFO] training using  Roh_Moo-hyun_0002.jpg\n",
      "Roh_Moo-hyun/Roh_Moo-hyun_0002.jpg was skipped and can't be used for training because detected 3 faces. \n",
      "[INFO] training using  Roh_Moo-hyun_0006.jpg\n",
      "[INFO] training using  Roh_Moo-hyun_0001.jpg\n",
      "[INFO] processing person [2/30] Gloria_Macapagal_Arroyo\n",
      "[INFO] training using  Gloria_Macapagal_Arroyo_0007.jpg\n",
      "[INFO] training using  Gloria_Macapagal_Arroyo_0009.jpg\n",
      "[INFO] training using  Gloria_Macapagal_Arroyo_0006.jpg\n",
      "[INFO] training using  Gloria_Macapagal_Arroyo_0001.jpg\n",
      "[INFO] training using  Gloria_Macapagal_Arroyo_0008.jpg\n",
      "[INFO] training using  Gloria_Macapagal_Arroyo_0002.jpg\n",
      "[INFO] training using  Gloria_Macapagal_Arroyo_0004.jpg\n",
      "[INFO] training using  Gloria_Macapagal_Arroyo_0010.jpg\n",
      "[INFO] training using  Gloria_Macapagal_Arroyo_0003.jpg\n",
      "[INFO] training using  Gloria_Macapagal_Arroyo_0005.jpg\n",
      "[INFO] processing person [3/30] Hans_Blix\n",
      "[INFO] training using  Hans_Blix_0002.jpg\n",
      "[INFO] training using  Hans_Blix_0005.jpg\n",
      "[INFO] training using  Hans_Blix_0010.jpg\n",
      "[INFO] training using  Hans_Blix_0008.jpg\n",
      "[INFO] training using  Hans_Blix_0003.jpg\n",
      "Hans_Blix/Hans_Blix_0003.jpg was skipped and can't be used for training because detected 2 faces. \n",
      "[INFO] training using  Hans_Blix_0007.jpg\n",
      "[INFO] training using  Hans_Blix_0001.jpg\n",
      "[INFO] training using  Hans_Blix_0009.jpg\n",
      "[INFO] training using  Hans_Blix_0004.jpg\n",
      "[INFO] training using  Hans_Blix_0006.jpg\n",
      "[INFO] processing person [4/30] Silvio_Berlusconi\n",
      "[INFO] training using  Silvio_Berlusconi_0008.jpg\n",
      "[INFO] training using  Silvio_Berlusconi_0001.jpg\n",
      "[INFO] training using  Silvio_Berlusconi_0003.jpg\n",
      "[INFO] training using  Silvio_Berlusconi_0005.jpg\n",
      "Silvio_Berlusconi/Silvio_Berlusconi_0005.jpg was skipped and can't be used for training because detected 2 faces. \n",
      "[INFO] training using  Silvio_Berlusconi_0007.jpg\n",
      "[INFO] training using  Silvio_Berlusconi_0010.jpg\n",
      "[INFO] training using  Silvio_Berlusconi_0006.jpg\n",
      "[INFO] training using  Silvio_Berlusconi_0004.jpg\n",
      "[INFO] training using  Silvio_Berlusconi_0009.jpg\n",
      "[INFO] training using  Silvio_Berlusconi_0002.jpg\n",
      "[INFO] processing person [5/30] Laura_Bush\n",
      "[INFO] training using  Laura_Bush_0008.jpg\n",
      "[INFO] training using  Laura_Bush_0005.jpg\n",
      "[INFO] training using  Laura_Bush_0001.jpg\n",
      "[INFO] training using  Laura_Bush_0007.jpg\n",
      "[INFO] training using  Laura_Bush_0004.jpg\n",
      "[INFO] training using  Laura_Bush_0010.jpg\n",
      "[INFO] training using  Laura_Bush_0003.jpg\n",
      "[INFO] training using  Laura_Bush_0009.jpg\n",
      "[INFO] training using  Laura_Bush_0006.jpg\n",
      "[INFO] training using  Laura_Bush_0002.jpg\n",
      "[INFO] processing person [6/30] John_Ashcroft\n",
      "[INFO] training using  John_Ashcroft_0007.jpg\n",
      "John_Ashcroft/John_Ashcroft_0007.jpg was skipped and can't be used for training because detected 3 faces. \n",
      "[INFO] training using  John_Ashcroft_0009.jpg\n",
      "[INFO] training using  John_Ashcroft_0002.jpg\n",
      "John_Ashcroft/John_Ashcroft_0002.jpg was skipped and can't be used for training because detected 2 faces. \n",
      "[INFO] training using  John_Ashcroft_0005.jpg\n",
      "[INFO] training using  John_Ashcroft_0006.jpg\n",
      "[INFO] training using  John_Ashcroft_0001.jpg\n",
      "[INFO] training using  John_Ashcroft_0008.jpg\n",
      "John_Ashcroft/John_Ashcroft_0008.jpg was skipped and can't be used for training because detected 2 faces. \n",
      "[INFO] training using  John_Ashcroft_0004.jpg\n",
      "[INFO] training using  John_Ashcroft_0010.jpg\n",
      "[INFO] training using  John_Ashcroft_0003.jpg\n",
      "John_Ashcroft/John_Ashcroft_0003.jpg was skipped and can't be used for training because detected 2 faces. \n",
      "[INFO] processing person [7/30] Tom_Ridge\n",
      "[INFO] training using  Tom_Ridge_0008.jpg\n",
      "[INFO] training using  Tom_Ridge_0003.jpg\n",
      "[INFO] training using  Tom_Ridge_0009.jpg\n",
      "[INFO] training using  Tom_Ridge_0010.jpg\n",
      "[INFO] training using  Tom_Ridge_0006.jpg\n",
      "[INFO] training using  Tom_Ridge_0001.jpg\n",
      "[INFO] training using  Tom_Ridge_0005.jpg\n",
      "[INFO] training using  Tom_Ridge_0002.jpg\n",
      "[INFO] training using  Tom_Ridge_0007.jpg\n",
      "[INFO] training using  Tom_Ridge_0004.jpg\n",
      "[INFO] processing person [8/30] Andre_Agassi\n",
      "[INFO] training using  Andre_Agassi_0003.jpg\n",
      "[INFO] training using  Andre_Agassi_0002.jpg\n",
      "[INFO] training using  Andre_Agassi_0004.jpg\n",
      "[INFO] training using  Andre_Agassi_0009.jpg\n",
      "[INFO] training using  Andre_Agassi_0005.jpg\n",
      "[INFO] training using  Andre_Agassi_0007.jpg\n",
      "[INFO] training using  Andre_Agassi_0001.jpg\n",
      "Andre_Agassi/Andre_Agassi_0001.jpg was skipped and can't be used for training because detected 2 faces. \n",
      "[INFO] training using  Andre_Agassi_0008.jpg\n",
      "[INFO] training using  Andre_Agassi_0010.jpg\n",
      "[INFO] training using  Andre_Agassi_0006.jpg\n",
      "[INFO] processing person [9/30] Vicente_Fox\n",
      "[INFO] training using  Vicente_Fox_0005.jpg\n",
      "[INFO] training using  Vicente_Fox_0009.jpg\n",
      "[INFO] training using  Vicente_Fox_0004.jpg\n",
      "[INFO] training using  Vicente_Fox_0001.jpg\n",
      "[INFO] training using  Vicente_Fox_0010.jpg\n",
      "[INFO] training using  Vicente_Fox_0002.jpg\n",
      "Vicente_Fox/Vicente_Fox_0002.jpg was skipped and can't be used for training because detected 2 faces. \n",
      "[INFO] training using  Vicente_Fox_0008.jpg\n",
      "[INFO] training using  Vicente_Fox_0003.jpg\n",
      "[INFO] training using  Vicente_Fox_0007.jpg\n",
      "[INFO] training using  Vicente_Fox_0006.jpg\n",
      "[INFO] processing person [10/30] Colin_Powell\n",
      "[INFO] training using  Colin_Powell_0010.jpg\n",
      "[INFO] training using  Colin_Powell_0009.jpg\n",
      "[INFO] training using  Colin_Powell_0004.jpg\n",
      "[INFO] training using  Colin_Powell_0007.jpg\n",
      "[INFO] training using  Colin_Powell_0006.jpg\n",
      "[INFO] training using  Colin_Powell_0008.jpg\n",
      "[INFO] training using  Colin_Powell_0005.jpg\n",
      "[INFO] training using  Colin_Powell_0002.jpg\n",
      "[INFO] training using  Colin_Powell_0003.jpg\n",
      "[INFO] training using  Colin_Powell_0001.jpg\n",
      "[INFO] processing person [11/30] Luiz_Inacio_Lula_da_Silva\n",
      "[INFO] training using  Luiz_Inacio_Lula_da_Silva_0007.jpg\n",
      "[INFO] training using  Luiz_Inacio_Lula_da_Silva_0004.jpg\n",
      "Luiz_Inacio_Lula_da_Silva/Luiz_Inacio_Lula_da_Silva_0004.jpg was skipped and can't be used for training because detected 2 faces. \n",
      "[INFO] training using  Luiz_Inacio_Lula_da_Silva_0005.jpg\n",
      "[INFO] training using  Luiz_Inacio_Lula_da_Silva_0003.jpg\n",
      "[INFO] training using  Luiz_Inacio_Lula_da_Silva_0010.jpg\n",
      "[INFO] training using  Luiz_Inacio_Lula_da_Silva_0006.jpg\n",
      "[INFO] training using  Luiz_Inacio_Lula_da_Silva_0009.jpg\n",
      "[INFO] training using  Luiz_Inacio_Lula_da_Silva_0008.jpg\n",
      "[INFO] training using  Luiz_Inacio_Lula_da_Silva_0002.jpg\n",
      "[INFO] training using  Luiz_Inacio_Lula_da_Silva_0001.jpg\n",
      "[INFO] processing person [12/30] Alejandro_Toledo\n",
      "[INFO] training using  Alejandro_Toledo_0008.jpg\n",
      "[INFO] training using  Alejandro_Toledo_0005.jpg\n",
      "[INFO] training using  Alejandro_Toledo_0009.jpg\n",
      "[INFO] training using  Alejandro_Toledo_0002.jpg\n",
      "[INFO] training using  Alejandro_Toledo_0006.jpg\n",
      "[INFO] training using  Alejandro_Toledo_0001.jpg\n",
      "[INFO] training using  Alejandro_Toledo_0003.jpg\n",
      "Alejandro_Toledo/Alejandro_Toledo_0003.jpg was skipped and can't be used for training because detected 2 faces. \n",
      "[INFO] training using  Alejandro_Toledo_0010.jpg\n",
      "[INFO] training using  Alejandro_Toledo_0007.jpg\n",
      "[INFO] training using  Alejandro_Toledo_0004.jpg\n",
      "[INFO] processing person [13/30] Megawati_Sukarnoputri\n",
      "[INFO] training using  Megawati_Sukarnoputri_0009.jpg\n",
      "[INFO] training using  Megawati_Sukarnoputri_0003.jpg\n",
      "Megawati_Sukarnoputri/Megawati_Sukarnoputri_0003.jpg was skipped and can't be used for training because detected 2 faces. \n",
      "[INFO] training using  Megawati_Sukarnoputri_0006.jpg\n",
      "[INFO] training using  Megawati_Sukarnoputri_0007.jpg\n",
      "[INFO] training using  Megawati_Sukarnoputri_0010.jpg\n",
      "[INFO] training using  Megawati_Sukarnoputri_0002.jpg\n",
      "[INFO] training using  Megawati_Sukarnoputri_0005.jpg\n",
      "[INFO] training using  Megawati_Sukarnoputri_0001.jpg\n",
      "[INFO] training using  Megawati_Sukarnoputri_0004.jpg\n",
      "[INFO] training using  Megawati_Sukarnoputri_0008.jpg\n",
      "[INFO] processing person [14/30] Lleyton_Hewitt\n",
      "[INFO] training using  Lleyton_Hewitt_0003.jpg\n",
      "[INFO] training using  Lleyton_Hewitt_0001.jpg\n",
      "[INFO] training using  Lleyton_Hewitt_0008.jpg\n",
      "[INFO] training using  Lleyton_Hewitt_0006.jpg\n",
      "[INFO] training using  Lleyton_Hewitt_0007.jpg\n",
      "[INFO] training using  Lleyton_Hewitt_0004.jpg\n",
      "[INFO] training using  Lleyton_Hewitt_0005.jpg\n",
      "[INFO] training using  Lleyton_Hewitt_0010.jpg\n",
      "[INFO] training using  Lleyton_Hewitt_0009.jpg\n",
      "[INFO] training using  Lleyton_Hewitt_0002.jpg\n",
      "[INFO] processing person [15/30] Unknown\n",
      "[INFO] training using  Zhong_Nanshan_0001.jpg\n",
      "[INFO] training using  Zumrati_Juma_0001.jpg\n",
      "[INFO] training using  Zhang_Wenkang_0001.jpg\n",
      "[INFO] training using  Zydrunas_Ilgauskas_0001.jpg\n",
      "[INFO] training using  Zhang_Ziyi_0003.jpg\n",
      "[INFO] training using  Zhang_Yimou_0001.jpg\n",
      "[INFO] training using  Zeng_Qinghong_0001.jpg\n",
      "[INFO] training using  Zorica_Radovic_0001.jpg\n",
      "[INFO] training using  Zurab_Tsereteli_0001.jpg\n",
      "[INFO] training using  Zoe_Ball_0001.jpg\n",
      "[INFO] processing person [16/30] George_W_Bush\n",
      "[INFO] training using  George_W_Bush_0005.jpg\n",
      "[INFO] training using  George_W_Bush_0004.jpg\n",
      "[INFO] training using  George_W_Bush_0010.jpg\n",
      "[INFO] training using  George_W_Bush_0003.jpg\n",
      "[INFO] training using  George_W_Bush_0001.jpg\n",
      "[INFO] training using  George_W_Bush_0009.jpg\n",
      "[INFO] training using  George_W_Bush_0007.jpg\n",
      "[INFO] training using  George_W_Bush_0006.jpg\n",
      "[INFO] training using  George_W_Bush_0008.jpg\n",
      "[INFO] training using  George_W_Bush_0002.jpg\n",
      "[INFO] processing person [17/30] Nestor_Kirchner\n",
      "[INFO] training using  Nestor_Kirchner_0009.jpg\n",
      "[INFO] training using  Nestor_Kirchner_0003.jpg\n",
      "[INFO] training using  Nestor_Kirchner_0004.jpg\n",
      "[INFO] training using  Nestor_Kirchner_0010.jpg\n",
      "[INFO] training using  Nestor_Kirchner_0005.jpg\n",
      "Nestor_Kirchner/Nestor_Kirchner_0005.jpg was skipped and can't be used for training because detected 2 faces. \n",
      "[INFO] training using  Nestor_Kirchner_0002.jpg\n",
      "Nestor_Kirchner/Nestor_Kirchner_0002.jpg was skipped and can't be used for training because detected 2 faces. \n",
      "[INFO] training using  Nestor_Kirchner_0001.jpg\n",
      "[INFO] training using  Nestor_Kirchner_0006.jpg\n",
      "[INFO] training using  Nestor_Kirchner_0007.jpg\n",
      "[INFO] training using  Nestor_Kirchner_0008.jpg\n",
      "[INFO] processing person [18/30] Alvaro_Uribe\n",
      "[INFO] training using  Alvaro_Uribe_0009.jpg\n",
      "[INFO] training using  Alvaro_Uribe_0002.jpg\n",
      "[INFO] training using  Alvaro_Uribe_0003.jpg\n",
      "[INFO] training using  Alvaro_Uribe_0007.jpg\n",
      "[INFO] training using  Alvaro_Uribe_0006.jpg\n",
      "[INFO] training using  Alvaro_Uribe_0008.jpg\n",
      "[INFO] training using  Alvaro_Uribe_0010.jpg\n",
      "[INFO] training using  Alvaro_Uribe_0001.jpg\n",
      "[INFO] training using  Alvaro_Uribe_0004.jpg\n",
      "[INFO] training using  Alvaro_Uribe_0005.jpg\n",
      "[INFO] processing person [19/30] Tony_Blair\n",
      "[INFO] training using  Tony_Blair_0009.jpg\n",
      "[INFO] training using  Tony_Blair_0003.jpg\n",
      "[INFO] training using  Tony_Blair_0010.jpg\n",
      "[INFO] training using  Tony_Blair_0002.jpg\n",
      "[INFO] training using  Tony_Blair_0008.jpg\n",
      "[INFO] training using  Tony_Blair_0005.jpg\n",
      "[INFO] training using  Tony_Blair_0007.jpg\n",
      "[INFO] training using  Tony_Blair_0004.jpg\n",
      "[INFO] training using  Tony_Blair_0001.jpg\n",
      "[INFO] training using  Tony_Blair_0006.jpg\n",
      "[INFO] processing person [20/30] Kofi_Annan\n",
      "[INFO] training using  Kofi_Annan_0001.jpg\n",
      "[INFO] training using  Kofi_Annan_0010.jpg\n",
      "Kofi_Annan/Kofi_Annan_0010.jpg was skipped and can't be used for training because detected 2 faces. \n",
      "[INFO] training using  Kofi_Annan_0003.jpg\n",
      "[INFO] training using  Kofi_Annan_0009.jpg\n",
      "[INFO] training using  Kofi_Annan_0008.jpg\n",
      "[INFO] training using  Kofi_Annan_0002.jpg\n",
      "[INFO] training using  Kofi_Annan_0004.jpg\n",
      "[INFO] training using  Kofi_Annan_0006.jpg\n",
      "[INFO] training using  Kofi_Annan_0005.jpg\n",
      "[INFO] training using  Kofi_Annan_0007.jpg\n",
      "Kofi_Annan/Kofi_Annan_0007.jpg was skipped and can't be used for training because detected 2 faces. \n",
      "[INFO] processing person [21/30] Arnold_Schwarzenegger\n",
      "[INFO] training using  Arnold_Schwarzenegger_0001.jpg\n",
      "[INFO] training using  Arnold_Schwarzenegger_0008.jpg\n",
      "[INFO] training using  Arnold_Schwarzenegger_0005.jpg\n",
      "Arnold_Schwarzenegger/Arnold_Schwarzenegger_0005.jpg was skipped and can't be used for training because detected 2 faces. \n",
      "[INFO] training using  Arnold_Schwarzenegger_0006.jpg\n",
      "[INFO] training using  Arnold_Schwarzenegger_0004.jpg\n",
      "[INFO] training using  Arnold_Schwarzenegger_0009.jpg\n",
      "[INFO] training using  Arnold_Schwarzenegger_0003.jpg\n",
      "[INFO] training using  Arnold_Schwarzenegger_0010.jpg\n",
      "[INFO] training using  Arnold_Schwarzenegger_0007.jpg\n",
      "Arnold_Schwarzenegger/Arnold_Schwarzenegger_0007.jpg was skipped and can't be used for training because detected 2 faces. \n",
      "[INFO] training using  Arnold_Schwarzenegger_0002.jpg\n",
      "[INFO] processing person [22/30] Jacques_Chirac\n",
      "[INFO] training using  Jacques_Chirac_0008.jpg\n",
      "[INFO] training using  Jacques_Chirac_0001.jpg\n",
      "[INFO] training using  Jacques_Chirac_0010.jpg\n",
      "Jacques_Chirac/Jacques_Chirac_0010.jpg was skipped and can't be used for training because detected 0 faces. \n",
      "[INFO] training using  Jacques_Chirac_0005.jpg\n",
      "[INFO] training using  Jacques_Chirac_0004.jpg\n",
      "[INFO] training using  Jacques_Chirac_0006.jpg\n",
      "Jacques_Chirac/Jacques_Chirac_0006.jpg was skipped and can't be used for training because detected 0 faces. \n",
      "[INFO] training using  Jacques_Chirac_0002.jpg\n",
      "[INFO] training using  Jacques_Chirac_0009.jpg\n",
      "[INFO] training using  Jacques_Chirac_0007.jpg\n",
      "[INFO] training using  Jacques_Chirac_0003.jpg\n",
      "[INFO] processing person [23/30] Donald_Rumsfeld\n",
      "[INFO] training using  Donald_Rumsfeld_0009.jpg\n",
      "[INFO] training using  Donald_Rumsfeld_0002.jpg\n",
      "[INFO] training using  Donald_Rumsfeld_0004.jpg\n",
      "[INFO] training using  Donald_Rumsfeld_0007.jpg\n",
      "[INFO] training using  Donald_Rumsfeld_0001.jpg\n",
      "[INFO] training using  Donald_Rumsfeld_0006.jpg\n",
      "[INFO] training using  Donald_Rumsfeld_0005.jpg\n",
      "[INFO] training using  Donald_Rumsfeld_0003.jpg\n",
      "[INFO] training using  Donald_Rumsfeld_0010.jpg\n",
      "[INFO] training using  Donald_Rumsfeld_0008.jpg\n",
      "[INFO] processing person [24/30] Ariel_Sharon\n",
      "[INFO] training using  Ariel_Sharon_0003.jpg\n",
      "[INFO] training using  Ariel_Sharon_0002.jpg\n",
      "[INFO] training using  Ariel_Sharon_0006.jpg\n",
      "[INFO] training using  Ariel_Sharon_0008.jpg\n",
      "[INFO] training using  Ariel_Sharon_0007.jpg\n",
      "[INFO] training using  Ariel_Sharon_0004.jpg\n",
      "Ariel_Sharon/Ariel_Sharon_0004.jpg was skipped and can't be used for training because detected 2 faces. \n",
      "[INFO] training using  Ariel_Sharon_0009.jpg\n",
      "[INFO] training using  Ariel_Sharon_0010.jpg\n",
      "[INFO] training using  Ariel_Sharon_0001.jpg\n",
      "[INFO] training using  Ariel_Sharon_0005.jpg\n",
      "[INFO] processing person [25/30] Hugo_Chavez\n",
      "[INFO] training using  Hugo_Chavez_0001.jpg\n",
      "[INFO] training using  Hugo_Chavez_0009.jpg\n",
      "[INFO] training using  Hugo_Chavez_0006.jpg\n",
      "[INFO] training using  Hugo_Chavez_0007.jpg\n",
      "[INFO] training using  Hugo_Chavez_0008.jpg\n",
      "[INFO] training using  Hugo_Chavez_0005.jpg\n",
      "[INFO] training using  Hugo_Chavez_0010.jpg\n",
      "[INFO] training using  Hugo_Chavez_0004.jpg\n",
      "[INFO] training using  Hugo_Chavez_0003.jpg\n",
      "[INFO] training using  Hugo_Chavez_0002.jpg\n",
      "[INFO] processing person [26/30] Serena_Williams\n",
      "[INFO] training using  Serena_Williams_0001.jpg\n",
      "[INFO] training using  Serena_Williams_0002.jpg\n",
      "[INFO] training using  Serena_Williams_0009.jpg\n",
      "[INFO] training using  Serena_Williams_0006.jpg\n",
      "[INFO] training using  Serena_Williams_0007.jpg\n",
      "[INFO] training using  Serena_Williams_0003.jpg\n",
      "[INFO] training using  Serena_Williams_0004.jpg\n",
      "[INFO] training using  Serena_Williams_0005.jpg\n",
      "[INFO] training using  Serena_Williams_0008.jpg\n",
      "[INFO] training using  Serena_Williams_0010.jpg\n",
      "[INFO] processing person [27/30] Jean_Chretien\n",
      "[INFO] training using  Jean_Chretien_0008.jpg\n",
      "[INFO] training using  Jean_Chretien_0010.jpg\n",
      "[INFO] training using  Jean_Chretien_0007.jpg\n",
      "[INFO] training using  Jean_Chretien_0005.jpg\n",
      "Jean_Chretien/Jean_Chretien_0005.jpg was skipped and can't be used for training because detected 2 faces. \n",
      "[INFO] training using  Jean_Chretien_0003.jpg\n",
      "[INFO] training using  Jean_Chretien_0009.jpg\n",
      "[INFO] training using  Jean_Chretien_0004.jpg\n",
      "[INFO] training using  Jean_Chretien_0001.jpg\n",
      "[INFO] training using  Jean_Chretien_0002.jpg\n",
      "[INFO] training using  Jean_Chretien_0006.jpg\n",
      "[INFO] processing person [28/30] Gerhard_Schroeder\n",
      "[INFO] training using  Gerhard_Schroeder_0001.jpg\n",
      "[INFO] training using  Gerhard_Schroeder_0009.jpg\n",
      "[INFO] training using  Gerhard_Schroeder_0007.jpg\n",
      "[INFO] training using  Gerhard_Schroeder_0002.jpg\n",
      "[INFO] training using  Gerhard_Schroeder_0003.jpg\n",
      "[INFO] training using  Gerhard_Schroeder_0010.jpg\n",
      "[INFO] training using  Gerhard_Schroeder_0006.jpg\n",
      "[INFO] training using  Gerhard_Schroeder_0008.jpg\n",
      "[INFO] training using  Gerhard_Schroeder_0004.jpg\n",
      "[INFO] training using  Gerhard_Schroeder_0005.jpg\n",
      "[INFO] processing person [29/30] Junichiro_Koizumi\n",
      "[INFO] training using  Junichiro_Koizumi_0006.jpg\n",
      "[INFO] training using  Junichiro_Koizumi_0001.jpg\n",
      "Junichiro_Koizumi/Junichiro_Koizumi_0001.jpg was skipped and can't be used for training because detected 2 faces. \n",
      "[INFO] training using  Junichiro_Koizumi_0009.jpg\n",
      "[INFO] training using  Junichiro_Koizumi_0004.jpg\n",
      "[INFO] training using  Junichiro_Koizumi_0005.jpg\n",
      "[INFO] training using  Junichiro_Koizumi_0002.jpg\n",
      "[INFO] training using  Junichiro_Koizumi_0010.jpg\n",
      "[INFO] training using  Junichiro_Koizumi_0007.jpg\n",
      "[INFO] training using  Junichiro_Koizumi_0008.jpg\n",
      "[INFO] training using  Junichiro_Koizumi_0003.jpg\n",
      "[INFO] processing person [30/30] Jennifer_Capriati\n",
      "[INFO] training using  Jennifer_Capriati_0002.jpg\n",
      "[INFO] training using  Jennifer_Capriati_0010.jpg\n",
      "[INFO] training using  Jennifer_Capriati_0008.jpg\n",
      "[INFO] training using  Jennifer_Capriati_0009.jpg\n",
      "[INFO] training using  Jennifer_Capriati_0005.jpg\n",
      "[INFO] training using  Jennifer_Capriati_0003.jpg\n",
      "[INFO] training using  Jennifer_Capriati_0007.jpg\n",
      "[INFO] training using  Jennifer_Capriati_0001.jpg\n",
      "[INFO] training using  Jennifer_Capriati_0006.jpg\n",
      "[INFO] training using  Jennifer_Capriati_0004.jpg\n",
      "Model successfully dumped to encodings.pickle\n"
     ]
    }
   ],
   "source": [
    "# Train multiple images per person\n",
    "# Find and recognize faces in an image using a SVC with scikit-learn\n",
    "# acquired from: https://github.com/ageitgey/face_recognition/blob/master/examples/face_recognition_svm.py\n",
    "# modified to fit personal use\n",
    "\n",
    "# set arguments\n",
    "args = {}\n",
    "args[\"dataset\"] = \"dataset\"\n",
    "args[\"detection_method\"] = \"hog\" # choose between 'hog', 'cnn'\n",
    "args[\"encodings\"] = \"encodings.pickle\"\n",
    "args[\"save_location\"] = args[\"encodings\"]\n",
    "print(\"[INFO] Imports & Arguments set!\")\n",
    "\n",
    "\"\"\"\n",
    "Structure:\n",
    "        <train_dir>/\n",
    "            <person_1>/\n",
    "                <person_1_face-1>.jpg\n",
    "                <person_1_face-2>.jpg\n",
    "                .\n",
    "                .\n",
    "                <person_1_face-n>.jpg\n",
    "           <person_2>/\n",
    "                <person_2_face-1>.jpg\n",
    "                <person_2_face-2>.jpg\n",
    "                .\n",
    "                .\n",
    "                <person_2_face-n>.jpg\n",
    "            .\n",
    "            .\n",
    "            <person_n>/\n",
    "                <person_n_face-1>.jpg\n",
    "                <person_n_face-2>.jpg\n",
    "                .\n",
    "                .\n",
    "                <person_n_face-n>.jpg\n",
    "\"\"\"\n",
    "\n",
    "import pickle\n",
    "import face_recognition\n",
    "from sklearn import svm\n",
    "import os\n",
    "\n",
    "# Training the SVC classifier\n",
    "\n",
    "# The training data would be all the face encodings from all the known images and the labels are their names\n",
    "knownEncodings = []\n",
    "knownNames = []\n",
    "\n",
    "# Training directory\n",
    "train_dir = os.listdir('./dataset/train/')\n",
    "\n",
    "# Loop through each person in the training directory\n",
    "for (i, person) in enumerate(train_dir):\n",
    "    print(\"[INFO] processing person [{}/{}]\".format(i + 1, len(train_dir)), person)\n",
    "    pix = os.listdir(\"./dataset/train/\" + person)\n",
    "    # Loop through each training image for the current person\n",
    "    for person_img in pix:\n",
    "        print(\"[INFO] training using \", person_img)\n",
    "        # Get the face encodings for the face in each image file\n",
    "        face = face_recognition.load_image_file(\"./dataset/train/\" + person + \"/\" + person_img)\n",
    "        face_bounding_boxes = face_recognition.face_locations(face)\n",
    "\n",
    "        #If training image contains exactly one face\n",
    "        if len(face_bounding_boxes) == 1:\n",
    "            face_enc = face_recognition.face_encodings(face)[0]\n",
    "            # Add face encoding for current image with corresponding label (name) to the training data\n",
    "            knownEncodings.append(face_enc)\n",
    "            knownNames.append(person)\n",
    "        else:\n",
    "            print(person + \"/\" + person_img + \" was skipped and can't be used for training because detected {} faces. \".format(len(face_bounding_boxes)))\n",
    "\n",
    "# # dump the facial encodings + names to disk\n",
    "# print(\"[INFO] serializing encodings...\")\n",
    "# data = {\"encodings\": knownEncodings, \"names\": knownNames}\n",
    "# f = open(args[\"encodings\"], \"wb\")\n",
    "# f.write(pickle.dumps(data))\n",
    "# f.close()\n",
    "# print(\"[INFO] Serialization Complete!\")\n",
    "\n",
    "# Create and train the SVC classifier\n",
    "model = svm.SVC(gamma='scale')\n",
    "model.fit(knownEncodings,knownNames)\n",
    "f = open(args[\"save_location\"], \"wb\")\n",
    "f.write(pickle.dumps(model))\n",
    "f.close()\n",
    "print(\"Model successfully dumped to \" + args[\"save_location\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Imports & Path set successfully!\n"
     ]
    }
   ],
   "source": [
    "# import required packages to run\n",
    "from imutils import paths # to get paths\n",
    "import cv2 # for face detection\n",
    "import os # for file system access\n",
    "import pickle # for storing embeddings\n",
    "import face_recognition # for embeddings\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "datasetPathAbs = os.path.abspath(args[\"dataset\"])\n",
    "\n",
    "if not os.path.exists(args[\"dataset\"]):\n",
    "    print(\"[WARN] \", datasetPathAbs, \"folder does not exist!\")\n",
    "    print(\"Create a folder there, and put subfolders named as the person's name (e.g. 'Long_Shun') and put images inside!\")\n",
    "    args[\"start_check\"] = \"false\"\n",
    "elif not os.access('.', os.W_OK):\n",
    "    print(\"[ERROR] Insufficient permission to write to current directory. You might want to check write permissions of the folder.\")\n",
    "    args[\"start_check\"] = \"false\"\n",
    "else:\n",
    "    print(\"[INFO] Imports & Path set successfully!\")\n",
    "    args[\"start_check\"] = \"true\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1.2: Detect & encode faces\n",
    "\n",
    "Detect face, encode them into the system, and then store the encodings inside the .pickle file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NOTE] Arguments set successfully\n",
      "[NOTE] Quantifying Faces\n",
      "[ERROR] No image found in ' /home/naina/Downloads/face-recognition-low-res-main/knn-version/dataset ' did you forgot to put the images in the dataset folder?\n"
     ]
    }
   ],
   "source": [
    "# set arguments\n",
    "args = {}\n",
    "args[\"dataset\"] = \"dataset\\dataset-full-res\"\n",
    "args[\"detection_method\"] = \"hog\" # choose between 'hog', 'cnn'\n",
    "args[\"encodings\"] = \"encodings.pickle\"\n",
    "args[\"start_check\"] = \"false\"\n",
    "\n",
    "print(\"[NOTE] Arguments set successfully\")\n",
    "\n",
    "# get input image paths\n",
    "print(\"[NOTE] Quantifying Faces\")\n",
    "imagePaths = list(paths.list_images(args[\"dataset\"]))\n",
    "if not imagePaths:\n",
    "    print(\"[ERROR] No image found in '\", datasetPathAbs, \"' did you forgot to put the images in the dataset folder?\")\n",
    "else:\n",
    "    # initialize the list of known encodings and known names\n",
    "    knownEncodings = []\n",
    "    knownNames = []\n",
    "\n",
    "    # FOR DEBUGGING\n",
    "    #print(imagePaths)\n",
    "\n",
    "    # loop over the image paths\n",
    "    for (i, imagePath) in enumerate(imagePaths):\n",
    "        # extract the person name from the image path\n",
    "        print(\"[INFO] processing image [{}/{}]\".format(i + 1, len(imagePaths)), imagePath)\n",
    "        name = imagePath.split(os.path.sep)[-2]\n",
    "\n",
    "        # load the input image and convert it from RGB (OpenCV ordering)\n",
    "        # to dlib ordering (RGB)\n",
    "        image = cv2.imread(imagePath)\n",
    "        rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        # detect the (x, y)-coordinates of the bounding boxes\n",
    "        # corresponding to each face in the input image\n",
    "        boxes = face_recognition.face_locations(rgb, model=args[\"detection_method\"])\n",
    "        print(len(boxes))\n",
    "        if len(boxes) == 0:\n",
    "            print(\"[WARN] Can't find faces! \", imagePath)\n",
    "            print(\"This image will be excluded from encoding, replace/delete the image to remove this warning.\")\n",
    "        elif len(boxes) > 1:\n",
    "            print(\"[WARN] Multiple faces found! \", imagePath)\n",
    "            print(\"This image will be excluded from encoding, replace/delete the image to remove this warning\")\n",
    "        else:\n",
    "            # compute the facial embedding for the face\n",
    "            encodings = face_recognition.face_encodings(rgb, boxes)\n",
    "\n",
    "            # loop over the encodings\n",
    "            for encoding in encodings:\n",
    "                # add each encoding + name to our set of known names and\n",
    "                # encodings\n",
    "                knownEncodings.append(encoding)\n",
    "                knownNames.append(name)\n",
    "                \n",
    "        for (top, right, bottom, left) in boxes:\n",
    "            # draw the predicted face name on the image\n",
    "            cv2.rectangle(rgb, (left, top), (right, bottom), (255, 0, 0), 1)\n",
    "        \n",
    "        # error checks\n",
    "        plt.imshow(rgb)\n",
    "        plt.title(imagePath)\n",
    "        plt.show()\n",
    "\n",
    "    # dump the facial encodings + names to disk\n",
    "    print(\"[INFO] serializing encodings...\")\n",
    "    data = {\"encodings\": knownEncodings, \"names\": knownNames}\n",
    "    f = open(args[\"encodings\"], \"wb\")\n",
    "    f.write(pickle.dumps(data))\n",
    "    f.close()\n",
    "    print(\"[INFO] Serialization Complete!\")\n",
    "    \n",
    "    # remove duplicates from list\n",
    "    dedupedNames = []\n",
    "    [dedupedNames.append(x) for x in knownNames if x not in dedupedNames]\n",
    "    \n",
    "    print(\"Serialized people: \", len(dedupedNames))\n",
    "    print(\"Serialized names: \", dedupedNames)\n",
    "    print(\"You can now open 2_recognize_faces_in_image Jupyter Notebook to detect the faces of the above person!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from fdet import MTCNN\n",
    "\n",
    "# detector = MTCNN()\n",
    "\n",
    "# # get input image paths\n",
    "# print(\"[NOTE] Quantifying Faces\")\n",
    "# imagePaths = list(paths.list_images(args[\"dataset\"]))\n",
    "\n",
    "# # initialize the list of known encodings and known names\n",
    "# knownEncodings = []\n",
    "# knownNames = []\n",
    "\n",
    "# # FOR DEBUGGING\n",
    "# #print(imagePaths)\n",
    "\n",
    "# # loop over the image paths\n",
    "# for (i, imagePath) in enumerate(imagePaths):\n",
    "#     # extract the person name from the image path\n",
    "#     print(\"[INFO] processing image {}/{} in {}\".format(i + 1, len(imagePaths), imagePath))\n",
    "#     name = imagePath.split(os.path.sep)[-2]\n",
    "\n",
    "#     # load the input image and convert it from RGB (OpenCV ordering)\n",
    "#     # to dlib ordering (RGB)\n",
    "#     image = cv2.imread(imagePath)\n",
    "#     rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "#     # detect the (x, y)-coordinates of the bounding boxes\n",
    "#     # corresponding to each face in the input image\n",
    "#     boxes = detector.detect(rgb)\n",
    "#     # print(boxes)\n",
    "#     # compute the facial embedding for the face\n",
    "#     # print(\"[NOTE]\", len(boxes), \" faces found in \", imagePath)\n",
    "#     for box in boxes:\n",
    "#         boxFormatted = [tuple(box.get('box'))]\n",
    "#         encodings = face_recognition.face_encodings(rgb,boxFormatted)\n",
    "#     # loop over the encodings\n",
    "#     for encoding in encodings:\n",
    "#         # add each encoding + name to our set of known names and\n",
    "#         # encodings\n",
    "#         knownEncodings.append(encoding)\n",
    "#         knownNames.append(name)\n",
    "\n",
    "# # dump the facial encodings + names to disk\n",
    "# print(\"[INFO] serializing encodings...\")\n",
    "# data = {\"encodings\": knownEncodings, \"names\": knownNames}\n",
    "# f = open(args[\"encodings\"], \"wb\")\n",
    "# f.write(pickle.dumps(data))\n",
    "# f.close()\n",
    "# print(\"[INFO] Complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Done?\n",
    "\n",
    "Open [Step 2: Recognize faces in image](2_recognize_faces_in_image.ipynb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
